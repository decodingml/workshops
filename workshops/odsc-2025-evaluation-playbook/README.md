<div align="center">
  <h1>LLM & RAG Evaluation Playbook</h1>
  <h3>A comprehensive guide for evaluating LLMs, RAG and agentic systems in production applications</h3>
  <p class="tagline"> <a href="https://odsc.com/speakers/llm-rag-evaluation-playbook-for-production-apps/">ODSC 2025 Webinar by Paul Iusztin</a> - Click to learn more about the webinar and speaker</p>
</div>

<div align="center">
    <a href="https://odsc.com/speakers/llm-rag-evaluation-playbook-for-production-apps/">
        <img src="./workshop_card.png" width="500" alt="Workshop Card" />
    </a>
</div>

This guide will help you set up and run the webinar, where we will explore the following topics:

- Add a prompt monitoring layer.
- Visualize the quality of the embeddings.
- Evaluate the context from the retrieval step used for RAG.
- Compute application-level metrics to expose hallucinations, moderation issues, and performance (using LLM-as-judges).
- Log the metrics to a prompt management tool to compare the experiments.
- Version the prompts

<div align="center">
    <a href="https://odsc.com/speakers/llm-rag-evaluation-playbook-for-production-apps/">
        <img src="./where_can_we_evaluate_a _RAG_agent.png" width="500" alt="Workshop Card" />
    </a>
</div>

# Choose Between the Solution or Template directory

Within the `solution` directory, we have the end-to-end solution, which is helpful for testing the code and verifying the solution.

We have the webinar code within the `template` directory, where you must implement parts of the code related to LLMOps and observability.

So, change the directory to `solution` or `template` depending on your goal.